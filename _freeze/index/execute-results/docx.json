{
  "hash": "ad67d6a32cd4b822086f1f574e269ef9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Near Infra-Red Spectroscopy Predicts Crude Protein in Hemp Grain\nexecute:\n  echo: false\nauthor:\n  - name: Ryan Crawford\n    # orcid: 0000-0002-0760-5497\n    corresponding: true\n    email: rvc3@cornell.edu\n    # roles:\n    #   - Investigation\n    #   - Project administration\n    #   - Software\n    #   - Visualization\n    affiliations:\n      - name: Cornell University\n        address: 126 Medicago Drive\n        city: Ithaca\n        state: NY\n        postal-code: 14853\n  - name: Jamie Crawford\n    # orcid: 0000-0002-0760-5497\n    corresponding: false\n    # roles:\n    #   - Investigation\n    #   - Project administration\n    #   - Software\n    #   - Visualization\n    affiliations:\n      - name: Cornell University\n        address: 126 Medicago Drive\n        city: Ithaca\n        state: NY\n        postal-code: 14853\n  - name: Lawrence B. Smart\n    # orcid: 0000-0002-7859-8394\n    corresponding: false\n    roles: []\n    affiliations:\n      - name: Cornell AgriTech\n        address: 102 Hedrick Hall\n        city: Geneva\n        state: NY\n        postal-code: 14456\n  - name: Virginia Moore\n    # orcid: 0000-0002-7859-8394\n    corresponding: false\n    roles: []\n    affiliations:\n      - name: Cornell University\n        address: 162 Emerson Hall\n        city: Ithaca\n        state: NY\n        postal-code: 14853    \nkeywords:\n  - Hemp\n  - Grain\n  - Spectroscopy\nabstract: |\n  Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nplain-language-summary: |\n  Earthquake data for the island of La Palma from the September 2021 eruption is found ...\nkey-points:\n  - A web scraping script was developed to pull data from the Instituto Geogràphico Nacional into a machine-readable form for analysis\n  - Earthquake events on La Palma are consistent with the presence of both mantle and crustal reservoirs.\ndate: last-modified\nbibliography:\n  - references.bib\n  - grateful-refs.bib\n\n# csl: apa.csl\n# citation:\n#   container-title: Earth and Space Science\n# number-sections: true\n---\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(data.table)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'data.table'\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(prospectr)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\u001b[34mprospectr version 0.2.7 -- cakes\u001b[39m\n\u001b[34mcheck the package repository at: https://github.com/l-ramirez-lopez/prospectr\u001b[39m\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(pls)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'pls'\n\nThe following object is masked from 'package:prospectr':\n\n    msc\n\nThe following object is masked from 'package:caret':\n\n    R2\n\nThe following object is masked from 'package:stats':\n\n    loadings\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n✔ broom        1.0.5      ✔ rsample      1.2.0 \n✔ dials        1.2.1      ✔ tune         1.1.2 \n✔ infer        1.0.6      ✔ workflows    1.1.4 \n✔ modeldata    1.3.0      ✔ workflowsets 1.0.1 \n✔ parsnip      1.2.0      ✔ yardstick    1.3.0 \n✔ recipes      1.0.10     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ data.table::between()    masks dplyr::between()\n✖ scales::discard()        masks purrr::discard()\n✖ dplyr::filter()          masks stats::filter()\n✖ data.table::first()      masks dplyr::first()\n✖ recipes::fixed()         masks stringr::fixed()\n✖ dplyr::lag()             masks stats::lag()\n✖ data.table::last()       masks dplyr::last()\n✖ caret::lift()            masks purrr::lift()\n✖ yardstick::precision()   masks caret::precision()\n✖ yardstick::recall()      masks caret::recall()\n✖ yardstick::sensitivity() masks caret::sensitivity()\n✖ yardstick::spec()        masks readr::spec()\n✖ yardstick::specificity() masks caret::specificity()\n✖ recipes::step()          masks stats::step()\n✖ data.table::transpose()  masks purrr::transpose()\n• Use tidymodels_prefer() to resolve common conflicts.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(nlme)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'nlme'\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(kableExtra)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n```\n\n\n:::\n:::\n\n\n\n**incomplete: may contain errors, run-ons, half-thoughts, etc.**\n\n## INTRODUCTION\n\nHemp (Cannabis sativa L.) is an annual crop with potential uses as a source of food or feed from grain, and bast fiber or hurd from the stalk. Hemp cultivars are commonly grown for one or both purposes and a cultivar may be referred to as a grain, fiber, or dual-purpose type. Because of protein's nutritional importance, the protein content of a grain crop is an prime consideration for researchers, producers, and consumers. Whole hemp grain typically contains approximately 20-30% protein [@ely_industrial_2022; @barta_proteomic_2024; @callaway2004]. Crude protein (CP) is often used as a proxy for the direct measurement of protein concentration and consists of the multiplication of nitrogen concentration by a conversion factor because measuring nitrogen concentration is relatively easy and cheap via laboratory assay [@hayes_measuring_2020].\n\nNear-infrared spectroscopy (NIRS) technology is rapid, non-destructive, and cheap, and consists of the measurement of NIR radiation reflected from a sample [@roberts_near-infrared_2004]. NIR spectra from many samples are related to laboratory values for components such as moisture, protein, fat, or fiber [@roberts_near-infrared_2004]. NIRS technology has been used since the 1970's to assess forage CP [@reeves_potential_2012; @williams_application_1975]. A NIRS calibration set often consists of samples from one species grown in many environments encompassing the range of expected values from the analyte or analytes [@chadalavada_nir_2022]. Partial least squares regression (PLSR) is a typical method used in the agricultural and food sciences to relate spectra to analyte [@roberts_near-infrared_2004]. PLSR calculates principal components (PCs) which relate to the dependent variable and summarize the spectra and uses a subset of PCs in order to fit the regression model. PLSR is commonly used in spectroscopy because it tends to work well with highly-correlated spectral data. Typically the number of principal components is chosen via cross-validation to avoid overfitting. **CITES FOR ALL OF THIS**\n\nA NIRS-scanned sample of undamaged grain may used for other purposes or it may planted as a seed. In wheat and corn, grain protein content has been shown to be heritable [@giancaspro_genetic_2019; @geyer_genetics_2022]. This suggests (at least potentially) that NIRS technology could serve as resource to more rapidly identify high CP hemp germplasm, enabling the delivery of higher CP hemp grain cultivars faster.\n\nFor this study, a benchtop NIR spectrometer was used to develop a model to predict CP content based on a data set of hemp grain representing multiple years, locations, and cultivars from grain and dual-purpose hemp types using PLSR.\n\n## MATERIALS AND METHODS\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# bckgrnd <- fread(\"./input_data/simplified_data/background_data_set.csv\") |> setDT()\n\n# spectra <- fread(\"./input_data/simplified_data/train_test_crude_protein.csv\")\n\n # bckgrnd[,in_ny:= ifelse(loc!=\"kentucky\", T, F)]\n \n# bg2 <- bckgrnd[loc!=\"kentucky\"]\n\n# extract indices of non-kentucky\n# bg_indices <- bckgrnd[loc!=\"kentucky\", which = T]\n\n# correct names in bg2--should be h-51, NOT hl-51\n\n# bg2[cultivar==\"hl-51\"]$cultivar <- \"h-51\"\n\n# check to see if i did the calc correctly\n\n# bg2[grepl(\"51\", cultivar),]\n\n #looks good \n# tab <-  table(bckgrnd$in_ny)\n\n\n# now take correct spectra filtering out stuff from KY\n\n# spectra_2 <- spectra[bg_indices]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# should be correct finalized data sets, \n\n# notably, switches sample 2 and 7 around\n\nfull_data <- fread(\"./input_data/final_data_set/full_hemp_data.csv\")\n\ntab <-  table(full_data$in_ny)\n```\n:::\n\n\n\n### Hemp Grain Sample Background\n\nSpectral data were obtained from whole (unground) hemp grain samples, harvested at maturity, collected from 2017 - 2021 from 18 cultivar trials in New York (NY) (NA samples). Grain samples were obtained by hand sampling or mechanical harvest and were cleaned of chaff and dried at 30 C for six days in a forced-air dryer. In total, 38 cultivars were represented in the data set. Cultivars were grain or dual-purpose types and included both commercially available and experimental material.\n\nAll cultivar trials were planted in randomized complete block design with each cultivar replicated four times. The 2017 data were comprised of samples from the same thirteen cultivars sampled from six NY locations. For those trials, grain was harvested from each plot individually and aggregated by cultivar within each trial. Four subsamples were drawn from each aggregated sample and scanned separately. These spectra were averaged at each 2 nm increment. All remaining samples from 2018-2021 were collected on a per-plot basis. All possible cultivars and possible locations were represented in 2017, but only a selected subset of cultivars and locations were represented in 2018-2021.\n\n### Spectral Data Collection and Preprocessing\n\nA benchtop NIR spectrometer (FOSS/ NIR FOSS/ NIR Systems model 5000) was used to obtain the spectra (FOSS North America, Eden Prairie, MN, USA). Spectra were collected every 2 nm from 1100-2498 nm and the logarithm of reciprocal reflectance was recorded.\n\nWINISI software version 1.02A (Infrasoft International, Port Matilda, PA, USA) was used to average the spectra in 2017, as well as to select samples for laboratory assay. Samples were selected according to their spectral distance from their nearest neighbor within the calibration data set with a cutoff of a distance of 0.6 H, where H is approximately equal to the squared Mahalanobis distance divided by the number of principal components used in the calculation [@garrido-varo_note_2019]. Prior to selection selection, spectra were preprocessed using SNV-detrend with settings 1,4,4,1 for the derivative, gap, smooth, and smooth 2 settings respectively.\n\n### Laboratory Validation\n\nLaboratory assays were performed by Dairy One Forage Laboratory (Ithaca, NY). For those assays, 1mm ground samples were analyzed by combustion using a CN628 or CN928 Carbon/Nitrogen Determinator. Samples from 2017 were aggregated as described above, but the remaining samples were not aggregated.\n\n### Model Development\n\nCalibration and validations sets were created by dividing the laboratory CP values into tertiles according to their percent CP in order to ensure that the range of CP values was present in both calibration and validation sets. Within each tertile, 75% of the samples were randomly assigned to the calibration set and the remaining 25% were assigned to the validation set. For each calibration set, models were developed in caret using PLSR. In fitting the model, the number of principal components was optimized over au grid search from 1-20. Model performance was evaluated with 25 iterations of bootstrapping and minimized root mean squared error (RMSE) in selecting the number of principal components in the final model [@kuhn2008].\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\npreproc_key <- fread(\"./input_data/final_data_set/preprocessing_key.csv\")\n\npreproc_key[,full_name:= c(\"Raw Spectra\", \"First Derivative\", \"Second Derivative\",\"Savitzky-Golay\", \"Gap-segment Derivative\",\n                           \"Standard Normal Variate\", \"Standard Normal Variate following Savitzky-Golay\", \"Standard Normal Variate-Detrend\", \"Multiplicative Scatter Correction\")]\n```\n:::\n\n\n\nInitially a number of common spectral preprocessing methods were tested by creating 100 calibration and validation sets as described above. Spectral data from those data sets were transformed by each of the following methods: 1) first derivative, 2) Savitzky-Golay (SG), 3) gap-segment derivative, 4) standard normal variate (SNV), 4) standard normal variate following Savitzky-Golay (SNV-SG), 5) SNV-detrend, and 6) multiplicative scatter correction. For each of these preprocessing methods, models were fit and predictions were made on the corresponding validation set (since there were 8 preprocessing methods, 8 separate models were fit for each of the 100 sets. The relationship between the predicted and actual values of the validation set were calculated via RMSE, R^2^ and Ratio of Performance to InterQuartile distance (RPIQ), three common model assessment metrics. Larger R^2^ and RPIQ, and smaller RMSE values are superior. Analyses of variance (ANOVA) were performed for each of these metrics in order to compare the preprocessing methods. For each ANOVA, each data set was considered as a subject and allowing different variances for each preprocessing method.\n\nOnce the most promising preprocessing method was identified, 1000 more data sets were created and analyzed via that method and performance on the validation sets was summarized with RMSE, R^2^, and RPIQ.\n\n### Additional software used\n\n<!-- Additional analyses were performed using R Statistical Software [@r_core_team_r_2024]. Data were tabulated and summarized using the tidyverse and data.table \\[@barrett_datatable_2024; @wickham_welcome_2019\\]. PLSR models were constructed using the pls package and validated using caret \\[@kuhn_building_2008; @liland_pls_2023\\]. -->\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ngrateful::cite_packages(output = \"paragraph\", out.dir = \".\")\n```\n\n::: {.cell-output-display}\nWe used R version 4.3.3 [@base] and the following R packages: caret v. 6.0.94 [@caret], data.table v. 1.15.2 [@datatable], emmeans v. 1.10.0 [@emmeans], kableExtra v. 1.4.0 [@kableExtra], knitr v. 1.45 [@knitr2014; @knitr2015; @knitr2023], lme4 v. 1.1.35.1 [@lme4], multcomp v. 1.4.25 [@multcomp], nlme v. 3.1.163 [@nlme2000; @nlme2023], pls v. 2.8.3 [@pls], prospectr v. 0.2.7 [@prospectr], randomForest v. 4.7.1.1 [@randomForest], rmarkdown v. 2.26 [@rmarkdown2018; @rmarkdown2020; @rmarkdown2024], skimr v. 2.1.5 [@skimr], tidymodels v. 1.1.1 [@tidymodels], tidyverse v. 2.0.0 [@tidyverse].\n:::\n:::\n\n\n\n## RESULTS AND DISCUSSION\n\n### Laboratory assay CP values\n\nLaboratory assay percent CP values are summarized in the following table. These are similar to the range of CP values observed in the literature, indicating an reasonable basis for a chemometric model. The CP values are left-skewed and two thirds of the samples contained more than 25% CP.\n\n\n\n::: {#tbl-lab-protein-vals .cell tbl-cap='Summary of Laboratory Assayed CP Values (Percent Dry Matter)'}\n\n```{.r .cell-code .hidden}\nmy_summary <- full_data$crude_protein |> skimr::skim()|> select(c(5:11)) |> \n  mutate_all(round, 1)\nnames(my_summary) <- c(\"mean\", \"sd\", \"minimum\", \"first quartile\", \"median\", \"third quartile\", \"maximum\") |> str_to_title()\n\nknitr::kable(my_summary)\n```\n\n::: {.cell-output-display}\n\n\n| Mean|  Sd| Minimum| First Quartile| Median| Third Quartile| Maximum|\n|----:|---:|-------:|--------------:|------:|--------------:|-------:|\n| 26.1| 2.5|    20.8|           23.9|   26.4|           28.2|    30.8|\n\n\n:::\n:::\n\n\n\n### Preprocessing methods comparison\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# \nmulti_metric <- metric_set(rmse, rsq, rpiq)\n\n\n# read data back in to work with it\n\nprep_key <- fread(\"./input_data/final_data_set/preprocessing_key.csv\")\n\nsims_key <- fread(\"./input_data/final_data_set/preprocessing_methods_test.csv\")\n\nlong_form <- merge(sims_key, prep_key, all.x = T)\n\n# now pull the metrics\n\n# long_form[, multi_metric(y, value), by = c(\"id\", \"preproc\")]\n\nsummaries <- long_form |> \n  filter(preproc!=\"second_derivative\") |> \n  group_by(id, preproc) |> \n  multi_metric(y, value)\n\n# # comparing methods over a series of metrics...\n# summaries_with_models <- summaries |> \n#   mutate(id = as.character(id)) |> \n#   nest(data = -.metric) |> \n#   mutate(mod = map(data, ~lme4::lmer(.estimate ~ preproc + (1|id), data = .x)),\n#          ems = map(mod, ~emmeans::emmeans(.x, \"preproc\") |> data.frame())\n#          )\n# \n# summaries_with_models_2 <- summaries_with_models |> \n#   select(1, ems) |> \n#   unnest(ems)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# actually, let's summarize via nlme\n# set our varident\nvf2 <- varIdent(form= ~ 1|preproc)\n\n# define custom contrasts\nfirst_part <- rep(1/7,7)\ncontrast_full <- append(first_part, -1, after = 3)\n\n\ncustom <- list(preprocess_vs_raw = contrast_full)\n\nnlme_summaries <- summaries |> \n  mutate(id = as.character(id)) |> \n  nest(data = -.metric) |> \n  mutate(\n    mod_standard = map(data, ~nlme::lme(.estimate ~ preproc, random = ~1|id, data = .x, method =\"ML\")),\n    \n    mod_varident = map(data, ~nlme::lme(.estimate ~ preproc, random = ~1|id, weights = vf2, data = .x, method =\"ML\")),\n    mod_compare = map2(mod_standard, mod_varident, ~anova(.x, .y)),\n         ems = map(mod_varident, ~emmeans::emmeans(.x, \"preproc\") |>multcomp::cld() |>  data.frame()), \n             ems2 = map(mod_varident, ~emmeans::emmeans(.x, \"preproc\")),\n                        contrast = map(ems2, ~emmeans::contrast(.x, custom))\n                                       )\n\n\n\nnlme_summaries_with_models_2 <- nlme_summaries |> \n  select(1, ems) |> \n  unnest(ems)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# add full names\n\nto_table <- nlme_summaries_with_models_2|> \n  left_join(preproc_key) |> \n  select('Preprocessing Method' = full_name, Metric = .metric, Estimate = emmean, SE) |> \n  mutate(Estimate = paste(format(round(Estimate, 2), nsmall = 2), \"±\", format(round(SE, 3), nsmall = 3)))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nJoining with `by = join_by(preproc)`\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ncontrasts <- nlme_summaries |> \n  select(.metric, contrast) |> \n  transmute(.metric, tidy_contrast = map(contrast, tidy)) |> \n  unnest(tidy_contrast)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# lead_lag summaries\nlead_lag <- nlme_summaries_with_models_2 |> select(1:4) |> \n  arrange(.metric, emmean)%>% \n  group_by(.metric) |> \n  mutate(lagged = lag(emmean)) %>% \n  mutate(pct_change = (emmean - lagged) / lagged)|> \n  mutate(lead = lead(emmean)) %>% \n  mutate(pct_change_lead = (emmean - lead) / lead)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# percent change for contrasts\n\nraw_contrast <- nlme_summaries_with_models_2 |> filter(preproc==\"raw\") |> \n  dplyr::select(1:3) |> \n  left_join(contrasts |> select(1,3,estimated_diff = 5)) |> \n  mutate(percent_difference = estimated_diff/(emmean+estimated_diff))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nJoining with `by = join_by(.metric)`\n```\n\n\n:::\n:::\n\n\n\nAll preprocessing methods outperformed raw spectral data @tbl-preproc. Averaged together, all preprocessed spectra were superior to raw spectra, with lower RMSE, and higher R^2^ and RPIQ values (significant at $\\alpha$ level \\<0.001). Preprocessing methods had -11.6 % lower RMSE, and had 3.1% higher R^2^ 7.4% higher RPIQ than unprocessed spectra.\n\nThe SNV-SG method had the lowest RMSE, highest R^2^, and highest RPIQ averaging over all iterations. SNV-SG RMSE averaged 1.4% lower, while R^2^ and RPIQ averaged 0.4% and 2.4% higher respectively than the next best preprocessing method (SG in both cases), but the difference between the best and second best method by metric were only statistically significant at $\\alpha$ \\<0.05 for RPIQ. RPIQ was devised to accurately reflect the spread of data in skewed populations [@bellon-maurel2010] and thus offers a robust metric for model assessment in this context, where the CP data are skewed. Therefore the superiority of SNV-SG as measured via RPIQ made it the best choice for the final model.\n\n\n\n::: {#tbl-preproc .cell tbl-cap='Evaluation of Preprocessing Methods by Metric ± Standard Error'}\n\n```{.r .cell-code .hidden}\n# printable table of results\n\nto_table2 <- to_table |> \n  select(1:3) |> \n  pivot_wider(names_from = Metric, values_from = Estimate) |> \n  arrange(rmse) |> \n  rename(RMSE = rmse, RPIQ = rpiq ) \n# names(to_table2)[3] <- \"$^{2}$\"\nto_table2|> \n  knitr::kable(col.names = c(\"Preprocessing Method\", \"RMSE\", \"$R^{2}$\", \"RPIQ\"))\n```\n\n::: {.cell-output-display}\n\n\n|Preprocessing Method                             |RMSE         |$R^{2}$      |RPIQ         |\n|:------------------------------------------------|:------------|:------------|:------------|\n|Standard Normal Variate following Savitzky-Golay |1.02 ± 0.012 |0.84 ± 0.004 |3.97 ± 0.076 |\n|Savitzky-Golay                                   |1.03 ± 0.012 |0.83 ± 0.004 |3.88 ± 0.072 |\n|First Derivative                                 |1.07 ± 0.013 |0.82 ± 0.004 |3.77 ± 0.075 |\n|Standard Normal Variate                          |1.12 ± 0.016 |0.80 ± 0.005 |3.61 ± 0.081 |\n|Gap-segment Derivative                           |1.12 ± 0.018 |0.81 ± 0.006 |3.60 ± 0.086 |\n|Standard Normal Variate-Detrend                  |1.13 ± 0.015 |0.80 ± 0.005 |3.55 ± 0.079 |\n|Multiplicative Scatter Correction                |1.17 ± 0.016 |0.79 ± 0.006 |3.47 ± 0.080 |\n|Raw Spectra                                      |1.22 ± 0.044 |0.79 ± 0.009 |3.42 ± 0.105 |\n\n\n:::\n:::\n\n\n\nThe preprocessing methods examined represent a portion of those available. As well, preprocessing methods tend to have a number of user-adjustable parameters whose various permutations were not tested. This subset of preprocessing methods and parameters nonetheless contained substantial variations in model quality, demonstrating the importance of the selection of an appropriate preprocessing method.\n\n### Final model development and summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nmodel_n_comp_statistics <- fread(\"./input_data/final_data_set/final_model_n_component_stats.csv\")\n\n# define a function to calculate percent difference\npct_lower <- function(x){\n  my_lag = data.table::shift(x, type = \"lag\")\n  round((x - my_lag)/my_lag*100,2)\n}\n\navg_change <- model_n_comp_statistics[,lapply(.SD, mean),.SDcols = 3:8, by= ncomp]\n\nchange_per_pc <- avg_change[, lapply(.SD, pct_lower), .SDcols = 2:7]\n```\n:::\n\n\n\nThe model improved most rapidly as the number of principal components increased from 1 to 7, with the inclusion of each additional PC being associated with a decrease in RMSE of 5-12% . From 8 to 12 PCs, model performance continued to improve, although gains were more modest (decrease in RMSE of 0.7-3%). With 13 or more PCs, performance gains were minimal and the relative ranks of the models tended to be stable @fig-model-calibration.\n\n\n\n::: {#cell-fig-model-calibration .cell}\n\n```{.r .cell-code .hidden}\nmodel_n_comp_statistics |> \n  ggplot(aes(as.factor(ncomp), RMSE)) + \n  geom_line(aes(group = id), alpha = 0.03) + \n  theme_classic() + \n  xlab(\"Crude Protein Model Number of Principal Components\") + \n  ylab(\"Crude Protein Model Root Mean Squared Error\")\n```\n\n::: {.cell-output-display}\n![Decreasing RMSE with increasing number of PCs](index_files/figure-docx/fig-model-calibration-1.png){#fig-model-calibration}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nmodel_final_predictions <- fread(\"./input_data/final_data_set/final_model_predictions.csv\")\n\nfinal_model_table <- model_final_predictions |> \n  group_by(id) |> \n  multi_metric(crude_protein, predicted_crude_protein) |> setDT()\n```\n:::\n\n\n\nFinal model performance was similar, but not identical to, that obtained during the initial comparison of preprocessing methods. The final models' mean RMSE  was 1.03, R^2^ was 0.83, and RPIQ was 3.89 (all calculated on the test sets). Despite the generally good model performance, a subset of poor models can be seen. For example, @fig-final-metric-boxplot shows 21 number of models with R^2^ below 0.7. \n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nskim_metrics <- final_model_table |> \n  group_by(.metric) |> \n  skimr::skim()\n```\n:::\n\n::: {#cell-fig-final-metric-boxplot .cell}\n\n```{.r .cell-code .hidden}\n# setnames(model_final_predictions, \"V1\", \"crude_protein\")\n\n\n\nfinal_model_table |>\n  mutate(metric2 = toupper(.metric)) |> \n  ggplot(aes(x = .metric, y = .estimate)) + \n  theme_classic() + geom_boxplot() + \n  facet_wrap(vars(metric2), scales = \"free\") +\n  xlab(\"Metric\") + ylab(\"Estimate\")+\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank())\n```\n\n::: {.cell-output-display}\n![Final model validation set performance (1000 iterations)](index_files/figure-docx/fig-final-metric-boxplot-1.png){#fig-final-metric-boxplot}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# add difference calculation between predicted and observed...\nmodel_final_predictions[,difference := predicted_crude_protein - crude_protein]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# my_cut <- cut(spectra_2$crude_protein, 3)\n# cut_dt <- data.table(ith_in_data_set = 1:149, cutpoints = my_cut)\n\n\n# revised_model_cutpoints\ncutpoints2 <- model_final_predictions |> \n  distinct(ith_in_data_set, crude_protein) |> \n  mutate(cutpoints = cut(crude_protein, 3))\n```\n:::\n\n::: {#cell-fig-validation_set_performance .cell}\n\n```{.r .cell-code .hidden}\n# \n# model_final_predictions |> \n#   left_join(cut_dt) |> \n#   ggplot(aes(fct_reorder(\n#     ith_in_data_set |> as.character(),\n#     crude_protein), crude_protein))+\n#   geom_point(aes(fct_reorder(\n#     ith_in_data_set |> as.character(),\n#     crude_protein), difference), alpha = 0.05, shape = 2) +\n#     geom_hline(yintercept = 0, linewidth = 2, lty = 2)+\n#   facet_wrap( ~cutpoints, scales = \"free_x\",\n#           labeller = as_labeller(c(\"(20.8,24.1]\" = \"Low (20.8 - 24.1)\",\n#                                    \"(24.1,27.5]\"= \"Medium (24.2 - 27.5)\",\n#                                    \"(27.5,30.8]\" =\"High (27.6-30.8)\"))) + \n#     theme_classic()+\n#   theme(\n#     axis.title.x = element_blank(),\n#     axis.text.x = element_blank())\n\nmodel_final_predictions |> \n  left_join(cutpoints2) |> \n  ggplot(aes(fct_reorder(\n    ith_in_data_set |> as.character(),\n    crude_protein), crude_protein))+\n  geom_point(aes(fct_reorder(\n    ith_in_data_set |> as.character(),\n    crude_protein), difference), alpha = 0.05, shape = 2) +\n  geom_hline(yintercept = 0, linewidth = 2, lty = 2) +\n  facet_wrap( ~cutpoints, scales = \"free_x\",\n              labeller = as_labeller(c(\"(20.8,24.1]\" = \"Low CP (20.8 - 24.1%)\",\n                                       \"(24.1,27.5]\"= \"Medium CP (24.2 - 27.5%)\",\n                                       \"(27.5,30.8]\" =\"High CP (27.6-30.8%)\"))) + \n  theme_classic()+\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank())\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nJoining with `by = join_by(crude_protein, ith_in_data_set)`\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Test set predictions versus normalized scores across hemp samples](index_files/figure-docx/fig-validation_set_performance-1.png){#fig-validation_set_performance}\n:::\n:::\n\n\n\nErrors tend to be lower at higher levels of actual CP\n\nThis study is limited in that it represents the creation of one model based upon spectra collected from one machine. NIRS calibrations can be unique to a particular machine, even if the machines compared are of the same model [@reeves2012]. As well, the calibration and validation sets are relatively small.\n\nThis research showed the promise of the use of NIRS in order to make predictions concerning %CP in hemp grain using PLS. Promising preprocessing methods were identified and a model was validated. Further research could refine the model by including more samples or by examining other predictive methods.\n\n## ACKNOWLEDGMENTS\n\n## SUPPLEMENTAL MATERIAL\n\n## OPTIONAL SECTIONS\n\n## REFERENCES\n\n## FIGURES AND TABLES\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\neruptions <- c(1492, 1585, 1646, 1677, 1712, 1949, 1971, 2021)\nn_eruptions <- length(eruptions)\n```\n:::\n\n::: {#cell-fig-timeline .cell}\n\n```{.r .cell-code .hidden}\npar(mar = c(3, 1, 1, 1) + 0.1)\nplot(eruptions, rep(0, n_eruptions), \n  pch = \"|\", axes = FALSE)\naxis(1)\nbox()\n```\n\n::: {.cell-output-display}\n![Timeline of recent earthquakes on La Palma](index_files/figure-docx/fig-timeline-1.png){#fig-timeline fig-alt='An event plot of the years of the last 8 eruptions on La Palma.'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\navg_years_between_eruptions <- mean(diff(eruptions[-n_eruptions]))\navg_years_between_eruptions\n```\n\n::: {.cell-output .cell-output-stdout .hidden}\n\n```\n[1] 79.83333\n```\n\n\n:::\n:::\n\n\n\nBased on data up to and including 1971, eruptions on La Palma happen every 79.8 years on average.\n\nStudies of the magma systems feeding the volcano, such as @marrero2019, have proposed that there are two main magma reservoirs feeding the Cumbre Vieja volcano; one in the mantle (30-40km depth) which charges and in turn feeds a shallower crustal reservoir (10-20km depth).\n\nEight eruptions have been recorded since the late 1400s (@fig-timeline).\n\nData and methods are discussed in @sec-data-methods.\n\nLet $x$ denote the number of eruptions in a year. Then, $x$ can be modeled by a Poisson distribution\n\n$$\np(x) = \\frac{e^{-\\lambda} \\lambda^{x}}{x !}\n$$ {#eq-poisson}\n\nwhere $\\lambda$ is the rate of eruptions per year. Using @eq-poisson, the probability of an eruption in the next $t$ years can be calculated.\n\n| Name                | Year |\n|---------------------|------|\n| Current             | 2021 |\n| Teneguía            | 1971 |\n| Nambroque           | 1949 |\n| El Charco           | 1712 |\n| Volcán San Antonio  | 1677 |\n| Volcán San Martin   | 1646 |\n| Tajuya near El Paso | 1585 |\n| Montaña Quemada     | 1492 |\n\n: Recent historic eruptions on La Palma {#tbl-history}\n\n@tbl-history summarises the eruptions recorded since the colonization of the islands by Europeans in the late 1400s.\n\n![Map of La Palma](images/la-palma-map.png){#fig-map}\n\nLa Palma is one of the west most islands in the Volcanic Archipelago of the Canary Islands (@fig-map).\n\n\n\n{{< embed notebooks/explore-earthquakes.qmd#fig-spatial-plot >}}\n\n\n\n\n\n@fig-spatial-plot shows the location of recent Earthquakes on La Palma.\n\n## Data & Methods {#sec-data-methods}\n\n## Conclusion\n\n## References {.unnumbered}\n\n::: {#refs}\n:::\n",
    "supporting": [
      "index_files/figure-docx"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}