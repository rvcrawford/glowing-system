---
title: Near Infrared Spectroscopy Predicts Crude Protein Concentration in Hemp Grain
execute:
  freeze: auto
  echo: false
author:
  - name: Ryan V. Crawford
    orcid: 0009-0006-3052-3269
    corresponding: true
    email: rvc3@cornell.edu
    # roles:
    #   - Investigation
    #   - Project administration
    #   - Software
    #   - Visualization
    affiliations:
      - name: Cornell University
        address: 126 Medicago Drive
        city: Ithaca
        state: NY
        postal-code: 14853
  - name: Jamie L. Crawford
    orcid: 0009-0002-2523-3479
    corresponding: false
    # roles:
    #   - Investigation
    #   - Project administration
    #   - Software
    #   - Visualization
    affiliations:
      - name: Cornell University
        address: 126 Medicago Drive
        city: Ithaca
        state: NY
        postal-code: 14853
  - name: Julie L. Hansen
    orcid: 0000-0001-7247-9186
    corresponding: falsec
    # roles:
    #   - Investigation
    #   - Project administration
    #   - Software
    #   - Visualization
    affiliations:
      - name: Cornell University
        address: 126 Medicago Drive
        city: Ithaca
        state: NY
        postal-code: 14853
  - name: Lawrence B. Smart
    orcid: 0000-0002-7812-7736
    corresponding: false
    roles: []
    affiliations:
      - name: Cornell AgriTech
        address: 102 Hedrick Hall
        city: Geneva
        state: NY
        postal-code: 14456
  - name: Virginia M. Moore
    orcid: 0000-0001-7888-3366
    corresponding: false
    roles: []
    affiliations:
      - name: Cornell University
        address: 162 Emerson Hall
        city: Ithaca
        state: NY
        postal-code: 14853    
keywords:
  - Hemp
  - Grain
  - Spectroscopy
abstract: |

      The protein concentration of hemp (*Cannabis sativa* L.) grain is of interest to researchers, producers, and consumers. This study was conducted to determine whether hemp grain can be non-destructively assayed for crude protein (CP) concentration using spectra obtained from near-infrared spectroscopy (NIRS) to build a prediction model for crude protein concentration using partial least squares regression (PLSR). One hundred and fourty-nine whole hemp grain samples were obtained from 18 cultivar trials in New York (NY) from 2017-2021. The samples' NIRS spectra were collected and the samples were ground and assayed by combustion. Seven potential preprocessing methods, as well as untransformed spectra, were tested using 100 training and testing set splits of the data and the best method was selected. That method was applied to 1000 additional splits of the data set. Model fit was evaluated using RMSE, R^2^, relative predicted deviation (RPD), and ratio of performance to interquartile distance (RPIQ). Once a preprocessing method was selected, the optimal number of model components and prediction performance on the testing sets were examined. A preprocessing method consisting of the standard normal variate transformation following a Savitzky-Golay filter had the lowest RMSE and the highest R^2^, RPD and RPIQ, with RPD and RPIQ 2.1%, and 2.4% higher than a Savitzky-Golay filter by itself (significant at $\alpha$ \<0.05). All preprocessing methods outperformed untransformed spectra. Optimal final models typically consisted of 12 components. Seventy-four percent of the 1000 final models had, at minimum, the ability to distinguish between high and low values of CP concentration, with 49% of the models capable of approximating quantitative prodiction. The models tested to overestimate CP concentration by 0.5% in the lowest tertile of samples and underestimate CP concentration by 0.4% in the highest tertile of samples. The worst-predicted samples tended to come from Geneva, NY, possibly as a result of the models' class imbalance (half of the samples were from Ithaca, NY while 28% were from Geneva). The research shows the promise that NIRS offers in the non-desctructive assay of CP concentration in hemp grain.
  
  
plain-language-summary: |
  A model was developed to predict percent crude protein in hemp grain using near infrared spectroscopy.
key-points:
  - Models were developed to predict crude protein concentration in hemp grain using near infrared spectroscopy.
  - Most models were able to distinguish between high and lower concentrations of crude protein. 
  - Models could be further optimized by including more samples and rectifying class imbalances between environments.
date: last-modified
bibliography: 
  - references.bib
  - grateful-refs.bib

csl: apa.csl
# citation:
#   container-title: Earth and Space Science
number-sections: true
# bibliography: references.bib
crossref:
  custom:
    - kind: float
      key: suppfig
      latex-env: suppfig
      reference-prefix: Figure S
      space-before-numbering: false
      latex-list-of-description: Supplementary Figure
---

```{r}
#| include: false

library(tidyverse)
library(data.table)
library(caret)
library(prospectr)
library(pls)
library(tidymodels)
library(nlme)
library(kableExtra)
```

**draft incorporates changes from Ginny, Larry, and Julie** CP, crude protein; NIR, near-infrared; NIRS, Near-infrared spectroscopy; NY, New York; PLSR, partial least squares regression; RPD, relative predicted deviation, RPIQ, ratio of performance to interquartile distance; SG, Savitzky-Golay; SNV, standard normal variate, SNV-SG, standard normal variate following Savitzky-Golay

## INTRODUCTION

Hemp (*Cannabis sativa* L.) is an annual crop with potential uses as a source of food or feed, derived from the grain, and fiber (bast or hurd), derived from the stalk. Hemp cultivars are commonly grown for one or both purposes and a cultivar may be called a grain, fiber, or dual-purpose type. Because of its nutritional importance, the protein concentration of a grain crop is a prime consideration for researchers, producers, and consumers. Whole hemp grain typically contains approximately 200-300 g kg^−1^ protein [@ely_industrial_2022; @barta_proteomic_2024; @callaway_hempseed_2004; @liu2023]. Crude protein is often used as a proxy for the direct measurement of protein concentration and consists of the multiplication of nitrogen concentration by a conversion factor, often 6.25 [@hayes_measuring_2020].

Near-infrared (NIR) spectroscopy (NIRS) technology is rapid, non-destructive, and inexpensive. It consists of the measurement of NIR radiation reflected and absorbed from a sample (the spectra) and the relation of the spectra to primary analytical values, typically obtained using wet chemistry assays, for components such as moisture, protein, fat, or fiber [@roberts_near-infrared_2004]. NIRS technology has been used since the 1970's to assess forage CP concentration [@reeves_potential_2012; @williams_application_1975]. A NIRS calibration set often consists of samples from diverse genotypes of one species grown in many environments encompassing the range of expected values from the analyte or analytes [@chadalavada_nir_2022]. Partial least squares regression (PLSR) is a typical method used in the agricultural and food sciences to relate spectra to analyte [@roberts_near-infrared_2004]. Partial least squares regression calculates components that maximize covariance between predictor and response variables. Partial least squares regression uses some number of components, often selected via cross-validation, to fit the regression model and is commonly used in spectroscopy because it tends to work well with highly correlated, noisy spectral data [@wold_pls-regression_2001].

A NIRS-scanned sample of whole grain may be used for other purposes besides the scan, including planting as a seed. In wheat and corn, grain protein content has been shown to be heritable [@giancaspro_genetic_2019; @geyer_genetics_2022]. This suggests that NIRS technology could serve as a resource to rapidly identify high concentration CP hemp germplasm, enabling the screening of germplasm as seed, before planting to the field, and facilitating the efficient development of high concentration CP hemp populations.

For this study, a benchtop NIR spectrometer was used to develop a model to predict CP concentration based on a data set of hemp grain representing multiple years, locations, and cultivars from grain and dual-purpose hemp types using PLSR.

## MATERIALS AND METHODS

```{r}
#| output: false

# bckgrnd <- fread("./input_data/simplified_data/background_data_set.csv") |> setDT()

# spectra <- fread("./input_data/simplified_data/train_test_crude_protein.csv")

 # bckgrnd[,in_ny:= ifelse(loc!="kentucky", T, F)]
 
# bg2 <- bckgrnd[loc!="kentucky"]

# extract indices of non-kentucky
# bg_indices <- bckgrnd[loc!="kentucky", which = T]

# correct names in bg2--should be h-51, NOT hl-51

# bg2[cultivar=="hl-51"]$cultivar <- "h-51"

# check to see if i did the calc correctly

# bg2[grepl("51", cultivar),]

 #looks good 
# tab <-  table(bckgrnd$in_ny)


# now take correct spectra filtering out stuff from KY

# spectra_2 <- spectra[bg_indices]

```

```{r}
# should be correct finalized data sets, 

# notably, switches sample 2 and 7 around

full_data <- fread("./input_data/final_data_set/full_hemp_data.csv")

tab <-  table(full_data$in_ny)


# read in stuff for supplemental table 
supp_tab <- fread("./input_data/final_data_set/cultivar_table_clean.csv")

```

### Hemp Grain Sample Background

Spectral data were obtained from whole (unground) hemp grain samples, harvested at maturity, collected from 2017--2021 from `r nrow(distinct(full_data |> filter(loc!= "kentucky"), harv_year, loc))` cultivar trials in New York (NY) (`r tab[[1]]` samples). Grain samples were obtained by hand sampling or mechanical harvest and were cleaned of chaff and dried at 30 C for six days in a forced-air dryer. All CP values were expressed as concentration dry matter. In total, `r nrow(full_data)` samples from `r full_data$cultivar |> unique() |> length()` cultivars were represented in the data set. Cultivars were grain or dual-purpose types and included both commercially available and experimental material. Seventy-eight samples were scanned and assayed in 2017, 19 in 2018, 24 in 2019, and 28 in 2021. More information about hemp cultivars and locations is available in Supplemental Table S1.

```{r}
#| label: tbl-hemp_provenance
#| tbl-cap: Tally of hemp cultivars and locations. Private cultivars are labeled "Cultivar1", "Cultivar2", etc., while experimental cultivars are labeled "Experimental1", "Experimental2", etc. 

supp_tab2 <- copy(supp_tab)

supp_tab2[,Cultivar:=toupper(cultivar2)]
names(supp_tab2) <- str_to_title(names(supp_tab2))
# get one that's sortable
st3 <- supp_tab2[,c(8,2:7)]
options(knitr.kable.NA = '')
knitr::kable(st3)

fwrite(st3, "./pastable_figures/supplemental_tableS1.csv")
```

All cultivar trials were planted in randomized complete block design with each cultivar replicated four times. The 2017 data were comprised of samples from the same 13 cultivars sampled from six NY locations. For those trials, grain was harvested from each plot individually and aggregated by cultivar within each trial. Four subsamples were drawn from each aggregated sample and scanned separately. These spectra were averaged at each 2 nm increment. All remaining samples from 2018-2021 were collected on a per-plot basis. All cultivars and locations were represented in 2017, but only a selected subset of cultivar-location combinations were represented in 2018-2021 because not all cultivars were planted everywhere and only a portion of these cultivar-location combinations were sampled, scanned, and assayed due to logistical constraints.

### Spectral Data Collection and Preprocessing

A benchtop NIR spectrometer (FOSS/ NIR FOSS/ NIR Systems model 5000) was used to obtain the spectra (FOSS North America, Eden Prairie, MN, USA). Spectra were collected every 2 nm from 1100-2498 nm and the logarithm of reciprocal reflectance was recorded. A 1/4 rectangular sample cup (5.7 cm × 4.6 cm) was used to scan the samples.

WINISI software version 1.02A (Infrasoft International, Port Matilda, PA, USA) was used to calculate the mean spectra in 2017 and to select samples for laboratory assay in all years. Samples were selected according to their spectral distance from their nearest neighbor within the data set with a cutoff of a distance of 0.6 H, where H is approximately equal to the squared Mahalanobis distance divided by the number of principal components used in the calculation [@garrido-varo_note_2019]. Prior to selection, spectra were preprocessed using SNV (standard normal variate)-detrend with settings 1,4,4,1 for the derivative, gap, smooth, and smooth-two settings respectively.

### Laboratory Validation

Laboratory assays were performed by Dairy One Forage Laboratory (Ithaca, NY). For those assays, 1 mm ground samples were analyzed by combustion using a CN628 or CN928 Carbon/Nitrogen Determinator. Samples from 2017 were aggregated as described above, but the remaining samples were not aggregated.

### R software and packages used

```{r}
grateful::cite_packages(output = "paragraph", out.dir = ".",pkgs = c("base", "data.table", "nlme", "tidyverse", "caret", "pls", "prospectr", "tidymodels", "emmeans", "skimr"))
```

### Model Development

Training and testing sets were created by dividing samples by their laboratory CP concentration values into tertiles to ensure that a representative range of values was present in both training and testing sets. Within each tertile, 75% of the samples were randomly assigned to the training set and the remaining 25% were assigned to the testing set. For each training set, models were developed in the caret package using PLSR. In fitting the model, the number of components was optimized over a grid search from 1-20. Model performance was evaluated with 25 iterations of bootstrapping and minimized RMSE in selecting the number of components in the final model.

```{r}
preproc_key <- fread("./input_data/final_data_set/preprocessing_key.csv")

preproc_key[,full_name:= c("Raw Spectra", "First Derivative", "Second Derivative","Savitzky-Golay", "Gap-segment Derivative",
                           "Standard Normal Variate", "Standard Normal Variate following Savitzky-Golay", "Standard Normal Variate-Detrend", "Multiplicative Scatter Correction")]
```

Initially a number of common spectral preprocessing methods were tested by creating 100 training and testing sets, as described above. Spectral data were transformed by each of the following methods: 1) first derivative; 2) Savitzky-Golay (SG) using the first derivative, third order polynomial, and a window of size five; 3) gap-segment derivative using the first derivative, a gap of 11, and a segment size of five; 4) SNV; 5) standard normal variate following Savitzky-Golay (SNV-SG) using the same SG parameters as above; 6) SNV-detrend with second order polynomial; and 7) multiplicative scatter correction. For comparison, models were also developed using untransformed spectra.

```{r}
n_preproc_methods <- (preproc_key$full_name) |> unique() |> length()-2

n_models_fit <- preproc_key$full_name |> unique() |> length()-1
```

For each of these preprocessing methods, models were fit and predictions were made on the corresponding testing set. Since there were seven preprocessing methods as well as untransformed spectra, eight separate models were fit for each of the 100 sets. The relationship between the predicted and actual values of the test set were calculated via RMSE, R^2^, relative predicted deviation (RPD), and ratio of performance to interquartile distance (RPIQ), four common model assessment metrics. Larger R^2^, RPD and RPIQ values and smaller RMSE values are best. The answer to the question of exactly which values constitute a "good" model varies depending upon the reference consulted, but for simplicity's sake the standard established for an acceptable model was R^2^ \> 0.80, an RPD greater than 2.5 and ideally greater than 3 ("good" to "excellent" quantitative prediction), and an RPIQ greater than 2.3 but ideally greater than 4.1 prediction on the testing set [@rawal_visible_2024; @luce_prediction_2017; @chadalavada_nir_2022].

Analyses of variance were performed for each of these metrics in order to compare preprocessing methods. For each ANOVA, each data set was considered as a subject and different variances were allowed for each preprocessing method. Once the most promising preprocessing method was identified, 1000 more training and testing sets were created, and models were developed with that method. Performance on the testing sets was summarized with RMSE, R^2^, RPD, and RPIQ. The pattern of errors, expressed as the difference between the actual and predicted values for a given sample, was examined.

## RESULTS AND DISCUSSION

### Laboratory assay CP values

```{r}
skewness <- function(x) {

    n <- length(x)

    mean_x <- mean(x)
    sd_x <- sqrt(sum((x - mean_x)^2) / (n))

    # z-transform x and assign to z
    z <- (x - mean_x) / sd_x

    # this is not an evaluation, the result is assigned to skewness
    skewness <- sum(z^3) / n

    # we need to evaulate skewness
    # we could write 
    # print(skewness)
    # or
    # return(skewness)
    skewness
}
```

Laboratory assay CP concentration values are summarized in @tbl-lab-protein-vals. These are similar to the range of values observed in the literature, indicating an reasonable basis for a chemometric model. The values were left-skewed (skewness of `r  round(skewness(full_data$crude_protein),2)`) and two thirds of the samples contained more than 250 g kg ^-1^ CP.

```{r}
#| label: tbl-lab-protein-vals
#| tbl-cap: Summary of Laboratory Assayed CP Values (g kg ^-1^)

my_summary <- full_data$crude_protein |> skimr::skim()|> select(c(5:11)) |> 
  mutate_all(function(x) round(x, 1)*10)
names(my_summary) <- c("mean", "sd", "minimum", "first quartile", "median", "third quartile", "maximum") |> str_to_title()

knitr::kable(my_summary)
```

### Preprocessing methods comparison

```{r}
# 
multi_metric <- metric_set(rmse, rsq, rpiq, rpd)


# read data back in to work with it

prep_key <- fread("./input_data/final_data_set/preprocessing_key.csv")

sims_key <- fread("./input_data/final_data_set/preprocessing_methods_test.csv")

long_form <- merge(sims_key, prep_key, all.x = T)

# now pull the metrics

# long_form[, multi_metric(y, value), by = c("id", "preproc")]

summaries <- long_form |> 
  filter(preproc!="second_derivative") |> 
  group_by(id, preproc) |> 
  multi_metric(y, value)

# # comparing methods over a series of metrics...
# summaries_with_models <- summaries |> 
#   mutate(id = as.character(id)) |> 
#   nest(data = -.metric) |> 
#   mutate(mod = map(data, ~lme4::lmer(.estimate ~ preproc + (1|id), data = .x)),
#          ems = map(mod, ~emmeans::emmeans(.x, "preproc") |> data.frame())
#          )
# 
# summaries_with_models_2 <- summaries_with_models |> 
#   select(1, ems) |> 
#   unnest(ems)

```

```{r}
# actually, let's summarize via nlme
# set our varident
vf2 <- varIdent(form= ~ 1|preproc)

# define custom contrasts
first_part <- rep(1/7,7)
contrast_full <- append(first_part, -1, after = 3)


custom <- list(preprocess_vs_raw = contrast_full)

nlme_summaries <- summaries |> 
  mutate(id = as.character(id)) |> 
  nest(data = -.metric) |> 
  mutate(
    mod_standard = map(data, ~nlme::lme(.estimate ~ preproc, random = ~1|id, data = .x, method ="ML")),
    
    mod_varident = map(data, ~nlme::lme(.estimate ~ preproc, random = ~1|id, weights = vf2, data = .x, method ="ML")),
    mod_compare = map2(mod_standard, mod_varident, ~anova(.x, .y)),
         ems = map(mod_varident, ~emmeans::emmeans(.x, "preproc") |>multcomp::cld() |>  data.frame()), 
             ems2 = map(mod_varident, ~emmeans::emmeans(.x, "preproc")),
                        contrast = map(ems2, ~emmeans::contrast(.x, custom))
                                       )



nlme_summaries_with_models_2 <- nlme_summaries |> 
  select(1, ems) |> 
  unnest(ems)
```

```{r}
# add full names

to_table <- nlme_summaries_with_models_2|> 
  left_join(preproc_key) |> 
  select('Preprocessing Method' = full_name, Metric = .metric, Estimate = emmean, SE) |> 
  mutate(Estimate = paste(format(round(Estimate, 2), nsmall = 2), "±", format(round(SE, 3), nsmall = 3)))
```

```{r}
contrasts <- nlme_summaries |> 
  select(.metric, contrast) |> 
  transmute(.metric, tidy_contrast = map(contrast, tidy)) |> 
  unnest(tidy_contrast)
  
```

```{r}
# lead_lag summaries
lead_lag <- nlme_summaries_with_models_2 |> select(1:4) |> 
  arrange(.metric, emmean)%>% 
  group_by(.metric) |> 
  mutate(lagged = lag(emmean)) %>% 
  mutate(pct_change = (emmean - lagged) / lagged)|> 
  mutate(lead = lead(emmean)) %>% 
  mutate(pct_change_lead = (emmean - lead) / lead)
```

```{r}
# percent change for contrasts

raw_contrast <- nlme_summaries_with_models_2 |> filter(preproc=="raw") |> 
  dplyr::select(1:3) |> 
  left_join(contrasts |> select(1,3,estimated_diff = 5)) |> 
  mutate(percent_difference = estimated_diff/(emmean+estimated_diff))
```

All preprocessing methods outperformed untransformed spectral data, as shown in @tbl-preproc. Averaged together, all preprocessed spectra were superior to untransformed spectra, with lower RMSE and higher R^2^, RPD and RPIQ values (significant at $\alpha$ level \<0.001). Preprocessing methods had `r abs(round(raw_contrast$percent_difference[[1]]*100,1))` % lower RMSE, and had `r round(raw_contrast$percent_difference[[2]] * 100,1)`% higher R^2^, `r round(raw_contrast$percent_difference[[4]] *100,1)`% higher RPD and `r round(raw_contrast$percent_difference[[3]] *100,1)`% higher RPIQ than unprocessed spectra. Preprocessed spectra also had lower standard errors than untransformed spectra.

The SNV-SG method had the lowest RMSE and the highest R^2^, RPD and RPIQ averaging over all iterations. SNV-SG's RMSE was 1.4% lower than the next best preprocessing method (SG), while SNV-SG's R^2^, RPD, and RPIQ were 0.4%, 2.1%, and 2.4% higher than SG respectively. However, the differences between the best and second-best methods by metric were only statistically significant at $\alpha$ \<0.05 for RPD and RPIQ. There is a long history of using RPD to evaluate chemometric models although the statistic has been criticized as inadequately reflecting the distribution of skewed populations, a situation which RPIQ was designed to address [@bellon-maurel_critical_2010]. In this study, the data were somewhat but not heavily skewed and RPD and RPIQ metrics agreed. The superiority of SNV-SG by these metrics made it the best choice for the final model.

```{r}
#| label: tbl-preproc
#| tbl-cap: Evaluation of Preprocessing Methods by Metric ± Standard Error

# printable table of results

to_table2 <- to_table |> 
  mutate(`Preprocessing Method` = 
           case_match(`Preprocessing Method`,
             "Raw Spectra"~ "Untransformed Spectra",
             .default=`Preprocessing Method`
           )) |> 
  select(1:3) |> 
  pivot_wider(names_from = Metric, values_from = Estimate) |> 
  arrange(rmse) |> 
  rename(RMSE = rmse, RPIQ = rpiq ) |> 
  select(1,2,3,5,4)
# names(to_table2)[3] <- "$^{2}$"
to_table2|> 
  knitr::kable(col.names = c("Preprocessing Method", "RMSE",  "$R^{2}$","RPD", "RPIQ"))

```

From the literature, these results are readily explained. Standard normal variate and SNV-detrend both correct light scatter, which is often a function of differences in particle size and sample packing density, although SNV-detrend is often used for densely-packed, powdered samples [@barnes_standard_1989]. SG is a smoothing filter that regresses on the signal over a series of windows, removing noise while preserving the signal's shape and features [@li_quantitative_2020; @luo_properties_2005]. Derivatives, here including SG, gap-segment, and first derivatives pretreatments may remove additive and multiplicative effects, but not necessarily light scatter; as well, derivatives may increase spectral noise [@rinnan_review_2009]. Here, hemp grain was neither powdered nor densely packed but samples were subject to light scatter and noise due to differences in particle size in the hemp grain.

<!-- The preprocessing methods examined represent a portion of those available. As well, these methods tend to have a number of user-adjustable parameters whose various permutations were not tested. This subset of preprocessing methods and parameters nonetheless contained substantial variations in model quality, demonstrating the importance of selecting an appropriate preprocessing method. -->

### Final model development and summary

```{r}
model_n_comp_statistics <- fread("./input_data/final_data_set/final_model_n_component_stats.csv")

# define a function to calculate percent difference
pct_lower <- function(x){
  my_lag = data.table::shift(x, type = "lag")
  round((x - my_lag)/my_lag*100,2)
}

avg_change <- model_n_comp_statistics[,lapply(.SD, mean),.SDcols = 3:8, by= ncomp]

change_per_pc <- avg_change[, lapply(.SD, pct_lower), .SDcols = 2:7]
```

The model improved most rapidly as the number of components increased from one to seven, with the inclusion of each additional component being associated with a decrease in RMSE of 5%-12%. From eight to 12 components, model performance continued to improve, although gains were more modest: there was a decrease in RMSE of 0.7%-3% with the inclusion of each additional component. With 13 or more components, performance gains were minimal and the relative ranks of the models tended to be stable (@fig-model-calibration).

```{r}
#| label: fig-model-calibration
#| fig-cap: Decreasing RMSE with increasing number of components for 1000 training sets


saveable_model <- model_n_comp_statistics |> 
  ggplot(aes(as.factor(ncomp), RMSE)) + 
  geom_line(aes(group = id), alpha = 0.03) + 
  theme_classic() + 
  xlab("Crude Protein Model Number of Components") + 
  ylab("Crude Protein Model Root Mean Squared Error")

ggsave("./pastable_figures/figure1.tiff") 
ggsave("./pastable_figures/figure1.png", dpi = 400) 

```

```{r}
model_final_predictions <- fread("./input_data/final_data_set/final_model_predictions.csv")

final_model_table <- model_final_predictions |> 
  group_by(id) |> 
  multi_metric(crude_protein, predicted_crude_protein) |> setDT()

```

```{r}
# make a final model table that spreads wide

ft2 <- final_model_table[,.estimator:=NULL] |> dcast(id~.metric, value.var = ".estimate")

corr_coeffs <- cor(ft2[,2:5])

# id the very worst fits.
low_cor_ids <- ft2[rsq<0.7,]$id


great_mods <- ft2[rpd>3&rpiq>4.1&rsq>0.8,]

good_mods <- ft2[rpd>2.5&rpd<3&rpiq<4.1&rpiq>2.3&rsq>0.8,]

ok_mods <- ft2[rpd>2.0&rpd<2.5&rpiq>2.3,]

poor_but_functional <-  ft2[rpd>1.5&rpd<2,]

all_mods <- list(great_mods, good_mods, ok_mods, poor_but_functional)

full_sum <- lapply(all_mods, nrow)
```

The performance of the final models on the testing sets were similar, but not identical to, those obtained during the initial comparison of preprocessing methods. The means of the final models were: RMSE = 1.03, R^2^ = 0.83, RPD = 2.44, and RPIQ = 3.89. Five percent of the models were "excellent" for quantitative prediction by both metrics, with RPD \> 3 and RPIQ \> 4.1, while an additional 11% of the models were "good" by both metrics (RPD range from 2.5--3.0, RPIQ range from 2.3--4.1). Forty-nine percent of the models had the ability to approximate quantitative prediction (RPD range from 2.0--2.5), and nine percent of the models were able to distinguish between higher and lower concentration CP values (RPD range from 1.5--2.0). Therefore, 74% of the models had, at minimum, the ability to distinguish between high and low values with 65% having, at minimum, the ability to approximate quantitative prediction. Despite the generally good model performance, a subset of poor models can be seen. For example, @fig-final-metric-boxplot shows 21 models with R^2^ below 0.7.

```{r}
skim_metrics <- final_model_table |> 
  group_by(.metric) |> 
  skimr::skim()
```

```{r}
#| label: fig-final-metric-boxplot
#| fig-cap: Final model testing set performance over 1000 iterations

# setnames(model_final_predictions, "V1", "crude_protein")
my_labeller <- as_labeller(c(RMSE="RMSE", RSQ="R^2", RPIQ="RPIQ", RPD = "RPD"),     default = label_parsed)


final_model_table |>
  mutate(metric2 = toupper(.metric)) |> 
  ggplot(aes(x = .metric, y = .estimate)) + 
  theme_classic() + geom_boxplot() + 
  facet_wrap(nrow = 1,
             vars(factor(metric2, levels = c("RMSE", "RSQ","RPD", "RPIQ"))),
             scales = "free", labeller = my_labeller) +
  xlab("Metric") + ylab("Estimate")+
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank())

ggsave("./pastable_figures/figure2.tiff") 
ggsave("./pastable_figures/figure2.png", dpi = 400) 

```

```{r}

# add difference calculation between predicted and observed...
model_final_predictions[,difference := predicted_crude_protein - crude_protein]

```

```{r}

# my_cut <- cut(spectra_2$crude_protein, 3)
# cut_dt <- data.table(ith_in_data_set = 1:149, cutpoints = my_cut)


# revised_model_cutpoints
cutpoints2 <- model_final_predictions |> 
  distinct(ith_in_data_set, crude_protein) |> 
  mutate(cutpoints = cut(crude_protein, 3, labels = c(
                           "Low~CP~(208-241~g~kg^{-1})",
                         "Medium~CP~(242-275~g~kg^{-1})",
                         "High~CP~(276-308~g~kg^{-1})")
         ))|> 
  arrange(crude_protein) |> 
  mutate(plot_order = 1:n())

```

```{r}
data_sum_by_sample <- model_final_predictions[,lapply(.SD, mean), .SDcols = c("crude_protein", "difference"), by = "ith_in_data_set"]
setorder(data_sum_by_sample, crude_protein)
temp_dat <- copy(data_sum_by_sample)
# create temporary ith
temp_dat[,tmp_ith:= 1:.N]
min_val <- min(temp_dat$crude_protein)
# center this
temp_dat[,adj_cp:= crude_protein - min_val]

lm_mod <- lm(difference~adj_cp, data = temp_dat) 
lm_mod_sum <- summary(lm_mod)

coefs <- coef(lm_mod)

preds <- predict(lm_mod, temp_dat)
ds_preds <- temp_dat[,c("ith_in_data_set")] |> cbind(preds)

ds_cutpoints <- merge(ds_preds, cutpoints2)|> 
  mutate(ct2 = factor(cutpoints, labels = c("AAA", "bold(BBB)", "italic(CCC)"))) |> setDT()
setorder(ds_cutpoints, plot_order)
```

```{r}
sd_mod <- model_final_predictions[,list(sd_diff = sd(difference), crude_protein = mean(crude_protein)), by = "ith_in_data_set"]
setorder(sd_mod, crude_protein)
temp_dat_sd <- copy(sd_mod)
# create temporary ith
temp_dat_sd[,tmp_ith:= 1:.N]
min_val_sd <- min(temp_dat_sd$crude_protein)
# center this
temp_dat_sd[,adj_cp:= crude_protein - min_val]

lm_mod_sd <- lm(sd_diff~adj_cp, data = temp_dat_sd) 
```

```{r}
# look at worst samples info...

worst_preds <- model_final_predictions[id%in%low_cor_ids]

worst <- worst_preds[,.N, by = ith_in_data_set][order(N, decreasing = T)][N>8]

troubles <- merge(worst, full_data[,1:7], all.x = T)
```

```{r}
cts <- ds_cutpoints[,mean(preds), by=cutpoints]
```

Finally, the pattern of test set errors was examined on a per-sample basis by calculating the difference between the actual and predicted values for the samples in the test sets @fig-validation_set_performance. A linear model was fit considering the mean estimated error for each sample where that sample was in the test set as compared to the sample's actual value. The models overestimated CP concentration by approximately `r round(cts$V1[1],2)`% in the lowest tertile and underestimated CP concentration by `r round(cts$V1[2],2)`% and `r round(cts$V1[3],2)`% in the middle and highest tertile, respectively. The variance of the errors did not increase appreciably as CP concentration increased.

```{r}
#| label: fig-validation_set_performance
#| fig-cap: Testing set prediction errors on a per-sample basis. Actual sample value set to zero and samples ranked from least to greatest actual CP concentration value 
# 

cutpoints2 <- cutpoints2 |> 
  mutate(ct2 = factor(cutpoints, labels = c("AAA", "bold(BBB)", "italic(CCC)")))

model_final_predictions |> 
  left_join(cutpoints2) |> 
  mutate(type = "Actual") |> 
  arrange(plot_order) |> 
  ggplot(aes(plot_order, crude_protein))+
  geom_point(aes(plot_order, difference, shape = type), alpha = 0.05) +
  geom_hline(yintercept = 0, linewidth = 2, lty = 2) +
  geom_point(data = ds_cutpoints |> mutate(type = "Predicted"), aes(plot_order, preds, shape = type))+
  scale_shape_manual("Error Type", values=c(2,3)) +
  facet_wrap( ~cutpoints,
              labeller =  label_parsed, scales = "free_x")+
              # labeller = as_labeller(
              #   c("(20.8,24.1]" = "Low CP (208-241 g kg ^-1)",
              # "(24.1,27.5]"= "Medium CP (242-275 g kg ^-1^)",
              #  "(27.5,30.8]" ="High CP (276-308 g kg ^-1^)")
              # ), scales = "free_x"
              # )+
              
      
  ylab("Crude Protein Predicted Percent Difference\nfrom Assayed Value")+
  theme_classic()+
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(), 
    legend.position=c(0.9, 0.8)) + 
  guides(shape= guide_legend(override.aes = list(alpha = 1)))

ggsave("./pastable_figures/figure3.tiff") 
ggsave("./pastable_figures/figure3.png", dpi = 400) 
```

```{r}
# look


# read in correct location key
locs2 <- fread("./input_data/final_data_set/corrected_location_key.csv")

# sort highest and lowest samples...
td2 <- temp_dat[order(abs(difference))]

# sort highest and lowest
td3 <- td2[!c(16:134),][,grouping:=rep(c("best_predicted_10pct", "worst_predicted_10_pct"), each = 15)]

# incorporate background

td4_bckgrd <- merge(td3, full_data[,1:7], all.x = T)

td_5 <- merge( td4_bckgrd,locs2, by.y ="loc", by.x= 'loc', all.x = T)


# make a prop table.
td_5_table <-  with(td_5, table(grouping, loc2))

test_for_errors <- merge( merge(td2, full_data[,1:7], all.x = T),locs2, by.y ="loc", by.x= 'loc', all.x = T)

error_mod <- lm(abs(difference)~loc2, data = test_for_errors)

error_by_loc <- emmeans::emmeans(error_mod, "loc2") |> multcomp::cld()
```

The 15 (10%) best and 15 worst predicted samples as measured by the mean absolute error of prediction were identified and their backgrounds examined. Overall, half of the samples in the data set came from Ithaca, NY ("Ithaca"), while 28% were collected from Geneva, NY ("Geneva") @tbl-hemp_provenance. However, of the 15 worst-predicted samples, nine were from Geneva, while three of the 15 best-predicted samples were from Geneva (by contrast, seven of the best-predicted and five of the worst-predicted samples came from Ithaca). Overall, samples from Geneva had the highest mean absolute error of prediction among locations, 61% greater than samples from Ithaca and 155% greater than samples from Freeville, NY (the only locations where more than 20 samples were assayed).

This study is limited in that it represents the creation of one model based upon spectra collected from one machine. NIRS calibrations can be unique to a particular machine, even if the machines compared are of the same model [@reeves_potential_2012]. As well, the testing and training sets are relatively small.

This research showed the promise of the use of NIRS in order to make predictions concerning CP concentration in hemp grain using PLS. Promising preprocessing methods were identified and a model was validated. Further research could refine the model by including more samples, particularly by rectifying the class imbalance between Geneva and Ithaca, identifying promising spectral regions, or by examining other predictive methods.

## ACKNOWLEDGMENTS

This work would not have been possible without the efforts of the field staff, undergraduate, and graduate students who planted, maintained, monitored and harvested these trials. Funding was provided by New York State through a grant from Empire State Development (AC477). We are grateful to those who provided seed for this project, including: Uniseeds, Verve Seeds, Winterfox Farms, International Hemp, Fiacre Seeds, and KonopiUS.

## CONFLICT OF INTEREST

The authors declare no conflict of interest.

## ORCID

## SUPPLEMENTAL MATERIAL

## OPTIONAL SECTIONS

## REFERENCES

## FIGURES AND TABLES
